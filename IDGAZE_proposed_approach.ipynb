{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DGAZE: Driver Gaze Mapping on the Road"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import os, sys\n",
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Input, Merge, Dropout\n",
    "from keras.layers import BatchNormalization\n",
    "from keras.optimizers import SGD, Adam, Adamax\n",
    "from keras.models import model_from_yaml\n",
    "from keras.regularizers import l1, l2\n",
    "from load_dataset import get_data, dataset\n",
    "from utils import print_metadata, get_dgaze_frames_count, split_data, plot_gaze_points, save_model, load_model\n",
    "\n",
    "from sklearn import preprocessing\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import copy\n",
    "import cv2 \n",
    "\n",
    "from numpy.random import seed\n",
    "seed(1)\n",
    "\n",
    "from tensorflow import set_random_seed\n",
    "set_random_seed(2)\n",
    "\n",
    "import random \n",
    "random.seed(3)\n",
    "\n",
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"2\"\n",
    "\n",
    "# from keras import backend as k\n",
    "# import tensorflow as tf\n",
    "# config = tf.ConfigProto(intra_op_parallelism_threads=1, inter_op_parallelism_threads=1,\n",
    "# allow_soft_placement=True, device_count = {'CPU': 1})\n",
    "# sess = tf.Session(graph=tf.get_default_graph(),config=config)\n",
    "# k.set_session(sess)\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 20/20 [00:13<00:00,  1.17it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Total frames in DGAZE dataset is 227178\n",
      "List of Features: ['left_eye', 'headpose_pupil', 'face_location', 'gaze_point', 'right_eye']\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "data_path = '/ssd_scratch/cvit/isha/eye_gaze_mapping/DGM_final2/dataset_samples_callibrated/'\n",
    "model_save = '/ssd_scratch/cvit/isha/DGAZE_pretrained_weights/weights_IDGAZE_proposed_approach2'\n",
    "drivers = os.listdir(data_path)\n",
    "ndrivers = len(drivers)\n",
    "frames_per_seq = 50\n",
    "sequences = 112\n",
    "batch_size = 8\n",
    "learning_rate = 0.001\n",
    "nepochs = 300\n",
    "\n",
    "# Driver_data is dict contatining drivers user1, user 2.....etc. For each driver, we have 112 sequences and for   \n",
    "# each sequence we have features like ['face_location', 'headpose_pupil', 'left_eye', 'gaze_point', 'right_eye'] \n",
    "driver_data = get_data(data_path, drivers, sequences, frames_per_seq)\n",
    "\n",
    "# Print the total numer of frames in the dataset\n",
    "get_dgaze_frames_count(driver_data, drivers)\n",
    "\n",
    "# Prints the DGAZE Metadata including list of drivers, sequences and features \n",
    "#print_metadata(driver_data, ['drivers', 'sequences', 'features'])\n",
    "print_metadata(driver_data, ['features'])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seq_range = np.arange(10, sequences+1)\n",
    "nsequences = len(seq_range)\n",
    "ndrivers = len(drivers)\n",
    "\n",
    "dsplit = [int(0.8*ndrivers),int(0.1*ndrivers), int(0.1*ndrivers)]\n",
    "gp_split = [int(0.7*nsequences),int(0.15*nsequences), int(0.15*nsequences)]\n",
    "data_split = split_data(drivers, seq_range, dsplit, gp_split)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Training dataset\n",
    "train = dataset(driver_data, data_split['drivers_train'], data_split['sequence_train'])\n",
    "\n",
    "# Validation dataset\n",
    "val = dataset(driver_data, data_split['drivers_val'], data_split['sequence_val'])\n",
    "\n",
    "# Test dataset\n",
    "test = dataset(driver_data, data_split['drivers_test'], data_split['sequence_test'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(train['left_eye'].shape, train['right_eye'].shape, train['headpose_pupil'].shape, \\\n",
    "      train['face_location'].shape, train['face_features'].shape, train['gaze_point'].shape)\n",
    "\n",
    "print(val['left_eye'].shape, val['right_eye'].shape, val['headpose_pupil'].shape, \\\n",
    "      val['face_location'].shape, val['face_features'].shape, val['gaze_point'].shape)\n",
    "\n",
    "print(test['left_eye'].shape, test['right_eye'].shape, test['headpose_pupil'].shape, \\\n",
    "      test['face_location'].shape, test['face_features'].shape, test['gaze_point'].shape)\n",
    "\n",
    "print(\"Total number of frames -->\",train['gaze_point'].shape[0] + val['gaze_point'].shape[0]\\\n",
    "      + test['gaze_point'].shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train['left_eye'].max()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot gaze point distribution on road image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_gaze_points(data_path, train['gaze_point'])\n",
    "plot_gaze_points(data_path, val['gaze_point'])\n",
    "plot_gaze_points(data_path, test['gaze_point'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Normalize face features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for d in data_split['drivers_val']:\n",
    "    data_calibrate = dataset(driver_data, [d], np.arange(12,13))\n",
    "    x = data_calibrate['face_location']\n",
    "    y = data_calibrate['headpose_pupil']\n",
    "\n",
    "    cap = cv2.VideoCapture(data_path + d + '/driver_view/sample_10.avi')\n",
    "    ret, frame = cap.read()\n",
    "    plt.figure()\n",
    "    cv2.rectangle(frame, (x[0,2], x[0,0]), (x[0,3], x[0,1]), (255, 255, 255), 6)\n",
    "    cv2.circle(frame,(int(y[0,6]), int(y[0,7])),3,(255,255,0),40)\n",
    "    cv2.circle(frame,(int(y[0,4]), int(y[0,5])),3,(255,255,0),40)\n",
    "    cv2.circle(frame,(int(y[0,9]), int(y[0,10])),3,(255,255,0),40)\n",
    "    plt.imshow(frame)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# scaler = preprocessing.MinMaxScaler()\n",
    "# train['face_features'] = scaler.fit_transform(train['face_features'])\n",
    "# val['face_features'] = scaler.transform(val['face_features'])\n",
    "# test['face_features'] = scaler.transform(test['face_features'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## I-DGAZE: Predicting driver gaze on road"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_lefteye = Sequential()\n",
    "model_lefteye.add(Conv2D(20, kernel_size=(3, 3),activation='relu',input_shape=(36,60,3)))\n",
    "model_lefteye.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model_lefteye.add(Dropout(0.5))\n",
    "model_lefteye.add(Conv2D(50, (3, 3), activation='relu',kernel_regularizer=l2(0.01), bias_regularizer=l2(0.01)))\n",
    "model_lefteye.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model_lefteye.add(Flatten())\n",
    "\n",
    "model_facefeatures = Sequential()\n",
    "model_facefeatures.add(Dense(16, activation ='relu', input_dim=(14)))\n",
    "\n",
    "model_merge = Sequential()\n",
    "model_merge.add(Merge([model_lefteye, model_facefeatures], mode = 'concat'))\n",
    "\n",
    "model_merge.add(Dense(512, activation='relu', kernel_regularizer=l2(0.01), bias_regularizer=l2(0.01)))\n",
    "model_merge.add(Dense(2))\n",
    "print(model_merge.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "opt = Adam(lr = 0.001)\n",
    "model_merge.compile(loss = 'mae', optimizer = opt )\n",
    "earlystopping = keras.callbacks.EarlyStopping(monitor = 'val_loss',min_delta = 1, patience =3, verbose =0, mode ='auto')\n",
    "\n",
    "history = model_merge.fit([train['left_eye'], train['face_features']], train['gaze_point'][:,:2], \\\n",
    "                validation_data= ([val['left_eye'], val['face_features']],val['gaze_point'][:,:2]),\n",
    "                epochs = nepochs, batch_size = 32, callbacks=[earlystopping], verbose=1, shuffle= True)\n",
    "\n",
    "save_model(model_save, model_merge)\n",
    "\n",
    "print(history.history.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Quantitative Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gaze_error(model, data):\n",
    "    scores = model.evaluate([data['left_eye'], data['face_features']], data['gaze_point'][:,:2])\n",
    "    return scores\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  ==> Before Calibration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_error = gaze_error(model_merge, train)\n",
    "val_error = gaze_error(model_merge, val)\n",
    "test_error = gaze_error(model_merge, test)\n",
    "    \n",
    "print(\"Train Error ==> \", train_error)\n",
    "print(\"Val Error ==> \",  val_error)\n",
    "print(\"Test Error ==> \" ,test_error)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ==> After Calibration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset for calibration\n",
    "test_calibrate = dataset(driver_data, data_split['drivers_test'], np.arange(1,10))\n",
    "#test_calibrate['face_features'] = scaler.transform(test_calibrate['face_features'])\n",
    "\n",
    "print(test_calibrate['left_eye'].shape, test_calibrate['right_eye'].shape, test_calibrate['headpose_pupil'].shape, \\\n",
    "      test_calibrate['face_location'].shape, test_calibrate['face_features'].shape, test_calibrate['gaze_point'].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "opt = Adam(lr=0.001)\n",
    "model_merge = load_model(model_save)\n",
    "model_merge.compile(loss = 'mae', optimizer = opt)\n",
    "\n",
    "model_merge.fit([test_calibrate['left_eye'], test_calibrate['face_features']], test_calibrate['gaze_point'][:,:2], \\\n",
    "                validation_data= ([test['left_eye'], test['face_features']],test['gaze_point'][:,:2]),\n",
    "                epochs = 300, batch_size = batch_size, callbacks=[earlystopping], verbose=1, shuffle= True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_error = gaze_error(model_merge, test)\n",
    "print(\"Test Error ==> \" ,test_error)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Qualitative Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from copy import copy \n",
    "\n",
    "def plot_data(driver_path, driver_data, model_save, driver, seq, n):\n",
    "        \n",
    "    video = driver_path + \"/\" + driver + \"/original_road_view/sample_\" + str(seq) + \".avi\"\n",
    "    driver_video = driver_path + \"/\" + driver + \"/driver_view/sample_\" + str(seq)+\".avi\"\n",
    "        \n",
    "    cap1 = cv2.VideoCapture(video) # road video\n",
    "    cap2 = cv2.VideoCapture(driver_video) # driver video\n",
    "\n",
    "    data = dataset(driver_data, [driver], np.arange(seq, seq+1))\n",
    "    data['face_features'] = scaler.transform(data['face_features'])\n",
    "    \n",
    "    opt = Adam(lr=0.0001)\n",
    "    model = load_model(model_save)\n",
    "    model.compile(loss='mae', optimizer=opt)\n",
    "    \n",
    "    if os.path.exists(video) and os.path.exists(driver_video):\n",
    "        \n",
    "        # Before Calibration\n",
    "        lefteye = data['left_eye']\n",
    "        righteye = data['right_eye']\n",
    "        face_features = data['face_features']\n",
    "        gaze_point = data['gaze_point']\n",
    "\n",
    "        pred = model.predict([lefteye, face_features]).astype(int)\n",
    "\n",
    "        \n",
    "        # After Calibration\n",
    "        data_calibrate = dataset(driver_data, [driver], np.arange(1,10))\n",
    "        data_calibrate['face_features'] = scaler.transform(data_calibrate['face_features'])\n",
    "\n",
    "        model = load_model(model_save)\n",
    "        model.compile(loss='mae', optimizer=opt)\n",
    "    \n",
    "        history = model.fit([data_calibrate['left_eye'], data_calibrate['face_features']],\n",
    "                                 data_calibrate['gaze_point'][:,:2], epochs=20, batch_size=batch_size, \n",
    "                                 verbose=1, shuffle= True)\n",
    "\n",
    "        pred_calibrate = model.predict([data['left_eye'], data['face_features']]).astype(int)\n",
    "        \n",
    "        \n",
    "        # Plot output\n",
    "        for i in range(int(n/2)):\n",
    "            ret, road_frame = cap1.read()\n",
    "            ret1, driver_frame = cap2.read()\n",
    "\n",
    "        frame1 = driver_frame\n",
    "        frame2 = copy(road_frame)\n",
    "        frame2 = cv2.circle(frame2, (data['gaze_point'][int(n/2),0], data['gaze_point'][int(n/2),1]), \\\n",
    "                            70, (0,255,0), -1)\n",
    "        frame3 = copy(road_frame)\n",
    "        frame3 = cv2.circle(frame3, (data['gaze_point'][int(n/2),0], data['gaze_point'][int(n/2),1]), \\\n",
    "                            70, (0,255,0), -1)\n",
    "        frame3 = cv2.circle(frame3, (pred[int(n/2),0], pred[int(n/2),1]), \\\n",
    "                            70, (0, 0,255), -1)\n",
    "        \n",
    "        frame4 = copy(road_frame)\n",
    "        for j in range(0, n):\n",
    "            frame4 = cv2.circle(frame4,(data['gaze_point'][j,0], data['gaze_point'][j,1]), 70, (0,255,0), -1 )\n",
    "            frame4 = cv2.circle(frame4,(pred[j,0], pred[j,1]), 70, (0,0,255), -1 )\n",
    "\n",
    "        frame5 = copy(road_frame)\n",
    "        for j in range(0, n):\n",
    "            frame5 = cv2.circle(frame5,(data['gaze_point'][j,0], data['gaze_point'][j,1]), 70, (0,255,0), -1 )\n",
    "            frame5 = cv2.circle(frame5,(pred_calibrate[j,0], pred_calibrate[j,1]), 70, (0,0,255), -1 )\n",
    " \n",
    "\n",
    "        frame_array =  np.concatenate((frame1, frame2, frame3, frame4, frame5), axis =1)\n",
    "        frame_array = cv2.resize(frame_array, (int(frame_array.shape[1]/4), int(frame_array.shape[0]/4)))\n",
    "\n",
    "        plt.figure(figsize=(20,10))\n",
    "        plt.axis('off')\n",
    "        plt.imshow(frame_array)\n",
    "        plt.show()\n",
    "\n",
    "        return frame_array\n",
    "    \n",
    "\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "users = data_split['drivers_test'] + data_split['drivers_val'] \n",
    "k =0\n",
    "for j in range(10,100,10):\n",
    "    k +=1\n",
    "  #  image_name = 'qualitative_results'+str(k)+'.png'\n",
    "    for i in range(len(users)):\n",
    "        frame_array = plot_data(data_path, driver_data, model_save, users[i], j+i, 50)\n",
    "\n",
    "#         print(frame_array.shape)\n",
    "#         if(i ==0):\n",
    "#             frame_array2 = frame_array\n",
    "#         else:\n",
    "#             frame_array2 = np.concatenate((frame_array2, frame_array), axis =0)\n",
    "#    # cv2.imwrite(image_name, frame_array2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_split['drivers_test'] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
